{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 import\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_extraction import FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = \"/Users/nayoung/Downloads/mali_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read_label 함수\n",
    "## read_json 함수\n",
    "## load_model 함수\n",
    "## train, evaluate 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 41\n",
    "\n",
    "# 정답 파일을 읽어 key: 파일이름, value: 1(또는 0)을 가지는 dictionary\n",
    "# dictionary에 파일이름을 검색하면 해당 파일의 정답을 알 수 있음\n",
    "def read_label_csv(path):\n",
    "    label_table = dict()\n",
    "    with open(path, \"r\", encoding='ISO-8859-1') as f:\n",
    "        for line in f.readlines()[1:]:\n",
    "            fname, label = line.strip().split(\",\")\n",
    "            label_table[fname] = int(label)\n",
    "    return label_table\n",
    "\n",
    "\n",
    "# json파일 불러오는 함수\n",
    "def read_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "    \n",
    "# 머신러닝 정의하는 함수\n",
    "def load_model(**kwargs):\n",
    "    if kwargs[\"model\"] == \"rf\":\n",
    "        return [\"rf\", RandomForestClassifier(random_state=kwargs[\"random_state\"], n_jobs=4)]\n",
    "    elif kwargs[\"model\"] == \"dt\":\n",
    "        return [\"dt\", DecisionTreeClassifier(random_state=kwargs[\"random_state\"])]\n",
    "    elif kwargs[\"model\"] == \"lgb\":\n",
    "        return [\"lgb\", LGBMClassifier(random_state=kwargs[\"random_state\"])]\n",
    "    elif kwargs[\"model\"] == \"svm\":\n",
    "        return [\"svm\", SVC(random_state=kwargs[\"random_state\"])]\n",
    "    elif kwargs[\"model\"] == \"lr\":\n",
    "        return [\"lr\", LogisticRegression(random_state=kwargs[\"random_state\"], n_jobs=-1)]\n",
    "    elif kwargs[\"model\"] == \"knn\":\n",
    "        return [\"knn\", KNeighborsClassifier(n_jobs=-1)]\n",
    "    elif kwargs[\"model\"] == \"adaboost\":\n",
    "        return [\"adaboost\", AdaBoostClassifier(random_state=kwargs[\"random_state\"])]\n",
    "    elif kwargs[\"model\"] == \"mlp\":\n",
    "        return [\"mlp\", MLPClassifier(random_state=kwargs[\"random_state\"])]\n",
    "    else:\n",
    "        print(\"Unsupported Algorithm\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# 머신러닝 모델을 선택하여 학습을 진행하는 함수\n",
    "# param X_train: 학습할 2차원 리스트 특징벡터\n",
    "# param y_train: 학습할 1차원 리스트 레이블 벡터\n",
    "# param model: 문자열, 선택할 머신러닝 알고리즘\n",
    "# return: 학습된 머신러닝 모델 객체\n",
    "def train(X_train, y_train, model):\n",
    "    model, clf = load_model(model=model, random_state=SEED)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return [model, clf]\n",
    "\n",
    "\n",
    "# 학습된 머신러닝 모델로 검증 데이터를 검증하는 함수\t\n",
    "# param X_test: 검증할 2차원 리스트 특징 벡터\n",
    "# param y_test: 검증할 1차원 리스트 레이블 벡터\n",
    "# param model: 학습된 머신러닝 모델 객체\n",
    "def evaluate(X_test, y_test, model):\n",
    "    predict = model[1].predict(X_test)\n",
    "    if model[0] == \"rf\":\n",
    "        print(\"RandomForest 정확도 : \", model[1].score(X_test, y_test))\n",
    "    elif model[0] == \"dt\":\n",
    "        print(\"DecisionTree 정확도 : \", model[1].score(X_test, y_test))\n",
    "    elif model[0] == \"lgb\":\n",
    "        print(\"LGBM 정확도 : \", model[1].score(X_test, y_test))\n",
    "    elif model[0] == \"svm\":\n",
    "        print(\"SVM 정확도 : \", model[1].score(X_test, y_test))\n",
    "    elif model[0] == \"lr\":\n",
    "        print(\"LogisticRegression 정확도 : \", model[1].score(X_test, y_test))\n",
    "    elif model[0] == \"knn\":\n",
    "        print(\"KNN 정확도 : \", model[1].score(X_test, y_test))\n",
    "    elif model[0] == \"adaboost\":\n",
    "        print(\"AdaBoost 정확도 : \", model[1].score(X_test, y_test))\n",
    "    elif model[0] == \"mlp\":\n",
    "        print(\"MLP 정확도 : \", model[1].score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXXNrGCUT2HQ"
   },
   "source": [
    "## 레이블 테이블 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_table = read_label_csv(local_dir + \"/학습데이터_정답.csv\")\n",
    "check_label_table = read_label_csv(local_dir + \"/검증데이터_정답.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpk-XEW9T2HT"
   },
   "source": [
    "## 특징 벡터 생성 예시\n",
    "- PEMINER 정보는 모두 수치형 데이터이므로 특별히 가공을 하지 않고 사용 가능\n",
    "- EMBER, PESTUDIO 정보는 가공해서 사용해야 할 특징들이 있음 (e.g. imports, exports 등의 문자열 정보를 가지는 데이터)\n",
    "- 수치형 데이터가 아닌 데이터(범주형 데이터)를 어떻게 가공할 지가 관건 >> 인코딩 (e.g. 원핫인코딩, 레이블인코딩 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeminerParser:\n",
    "    def __init__(self, path):\n",
    "        self.report = read_json(path)\n",
    "        self.vector = []\n",
    "    \n",
    "    def process_report(self):\n",
    "        '''\n",
    "            전체 데이터 사용        \n",
    "        '''\n",
    "        \n",
    "        self.vector = [value for _, value in sorted(self.report.items(), key=lambda x: x[0])]\n",
    "        return self.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmberParser:\n",
    "    '''\n",
    "        예제에서 사용하지 않은 특징도 사용하여 벡터화 할 것을 권장\n",
    "    '''\n",
    "    def __init__(self, path):\n",
    "        self.report = read_json(path)\n",
    "        self.vector = []\n",
    "    \n",
    "    def get_histogram_info(self):\n",
    "        histogram = np.array(self.report[\"histogram\"])\n",
    "        total = histogram.sum()\n",
    "        vector = histogram / total\n",
    "        return vector.tolist()\n",
    "    \n",
    "    def get_string_info(self):\n",
    "        strings = self.report[\"strings\"]\n",
    "\n",
    "        hist_divisor = float(strings['printables']) if strings['printables'] > 0 else 1.0\n",
    "        vector = [\n",
    "            strings['numstrings'], \n",
    "            strings['avlength'], \n",
    "            strings['printables'],\n",
    "            strings['entropy'], \n",
    "            strings['paths'], \n",
    "            strings['urls'],\n",
    "            strings['registry'], \n",
    "            strings['MZ']\n",
    "        ]\n",
    "        vector += (np.asarray(strings['printabledist']) / hist_divisor).tolist()\n",
    "        return vector\n",
    "    \n",
    "    def get_general_file_info(self):\n",
    "        general = self.report[\"general\"]\n",
    "        vector = [\n",
    "            general['size'], general['vsize'], general['has_debug'], general['exports'], general['imports'],\n",
    "            general['has_relocations'], general['has_resources'], general['has_signature'], general['has_tls'],\n",
    "            general['symbols']\n",
    "        ]\n",
    "        return vector\n",
    "\n",
    "    # header, section, imports, exports 데이터 ember json 파일 가공\n",
    "    def get_header_file_info(self):\n",
    "        headers = self.report['header']\n",
    "        header_coff = [\n",
    "            'timestamp', 'machine', 'characteristics'\n",
    "        ]\n",
    "        header_optional = [\n",
    "            'subsystem', 'dll_characteristics', 'magic', 'major_image_version', 'minor_image_version',\n",
    "            'major_linker_version', 'minor_linker_version', 'major_operating_system_version', 'minor_operating_system_version', \n",
    "            'major_subsystem_version', 'minor_subsystem_version', 'sizeof_code', 'sizeof_headers', 'sizeof_heap_commit'\n",
    "        ]\n",
    "        vector = [\n",
    "            headers['coff'][header_coff[0]],\n",
    "            # headers['coff'][header_coff[1]],\n",
    "            # headers['coff'][header_coff[2]],\n",
    "            # headers['optional'][header_optional[0]],\n",
    "            # headers['optional'][header_optional[1]],\n",
    "            # headers['optional'][header_optional[2]],\n",
    "            headers['optional'][header_optional[3]],\n",
    "            headers['optional'][header_optional[4]],\n",
    "            headers['optional'][header_optional[5]],\n",
    "            headers['optional'][header_optional[6]],\n",
    "            headers['optional'][header_optional[7]],\n",
    "            headers['optional'][header_optional[8]],\n",
    "            headers['optional'][header_optional[9]],\n",
    "            headers['optional'][header_optional[10]],\n",
    "            headers['optional'][header_optional[11]],\n",
    "            headers['optional'][header_optional[12]],\n",
    "            headers['optional'][header_optional[13]]\n",
    "        ]\n",
    "        return vector\n",
    "\n",
    "    # def get_section_info(self):\n",
    "    #     section = self.report['section']\n",
    "    #     sections = self.report['sections']\n",
    "    #     section_entry = [\n",
    "    #         len(sections), # total number of sections\n",
    "    #         # number of sections with nonzero size\n",
    "    #         sum(1 for s in sections if s['size'] == 0),\n",
    "    #         # number of sections with an empty name\n",
    "    #         sum(1 for s in sections if s['name'] == \"\"),\n",
    "    #     ]\n",
    "    #     sections_names = [s['name'] for s in sections]\n",
    "    #     sections_sizes = [(s['name'], s['size']) for s in sections]\n",
    "    #     sections_entropy = [(s['name'], s['entropy']) for s in sections]\n",
    "    #     sections_vsize = [(s['name'], s['vsize']) for s in sections]\n",
    "    #     sections_props = [p for s in sections for p in s['props'] if s['name'] == section['entry']]\n",
    "    #     vector = [\n",
    "    #         section_entry, sections_names, sections_sizes, sections_entropy, sections_vsize, sections_props\n",
    "    #     ]\n",
    "    #     return vector\n",
    "\n",
    "    def get_imports_info(self):\n",
    "        imports = self.report[\"imports\"]\n",
    "        libraries = list(set([l.lower() for l in imports.keys()]))\n",
    "        libraries_hashed = FeatureHasher(256, input_type=\"string\").transform([libraries]).toarray()[0]\n",
    "        import_function = [lib.lower() + ':' + e for lib, elist in imports.items() for e in elist]\n",
    "        import_function_hashed = FeatureHasher(1024, input_type=\"string\").transform([import_function]).toarray()[0]\n",
    "        vector = [\n",
    "            libraries_hashed, import_function_hashed\n",
    "        ]\n",
    "        return vector\n",
    "        \n",
    "    def get_exports_info(self):\n",
    "        exports = self.report[\"exports\"]\n",
    "        exports_hashed = FeatureHasher(128, input_type=\"string\").transform([exports]).toarray()[0]\n",
    "        vector = [ exports_hashed ]\n",
    "        return vector\n",
    "\n",
    "    # def get_datadirectories(self):\n",
    "    #     datadirectories = self.report[\"datadirectories\"]\n",
    "    #     names = [\n",
    "    #         \"EXPORT_TABLE\", \"IMPORT_TABLE\", \"RESOURCE_TABLE\", \"EXCEPTION_TABLE\", \"CERTIFICATE_TABLE\",\n",
    "    #         \"BASE_RELOCATION_TABLE\", \"DEBUG\", \"ARCHITECTURE\", \"GLOBAL_PTR\", \"TLS_TABLE\", \"LOAD_CONFIG_TABLE\",\n",
    "    #         \"BOUND_IMPORT\", \"IAT\", \"DELAY_IMPORT_DESCRIPTOR\", \"CLR_RUNTIME_HEADER\"\n",
    "    #     ]\n",
    "\n",
    "    def process_report(self):\n",
    "        vector = []\n",
    "        vector += self.get_general_file_info()\n",
    "        vector += self.get_histogram_info()\n",
    "        vector += self.get_string_info()\n",
    "        '''\n",
    "            특징 추가\n",
    "        '''\n",
    "        # vector += self.get_header_file_info()\n",
    "        # vector += self.get_section_info()\n",
    "        # vector += self.get_imports_info()\n",
    "        # vector += self.get_exports_info()\n",
    "        # vector += self.get_datadirectories()\n",
    "        \n",
    "        return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PestudioParser:\n",
    "    '''\n",
    "        사용할 특징을 선택하여 벡터화 할 것을 권장\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.report = read_json(path)\n",
    "        self.vector = []\n",
    "    \n",
    "    def process_report(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5VRDTBbT2Hd"
   },
   "source": [
    "## 데이터 벡터 구성\n",
    "- 특징 벡터 구성은 2차원이 되어야함 e.g.  [vector_1, vector_2, ..., vector_n]\n",
    "\n",
    "- 각 벡터는 1차원 리스트, 벡터 크기는 모두 같아야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습데이터 X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 558), (20000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습데이터\n",
    "peminer_url = (local_dir + '/PEMINER/학습데이터/')\n",
    "ember_url = (local_dir + '/EMBER/학습데이터/')\n",
    "pestudio_url = (local_dir + '/PESTUDIO/학습데이터/')\n",
    "\n",
    "# 데이터의 특징 벡터 모음(2차원 리스트) : X\n",
    "# 데이터의 레이블 모음(1차원 리스트) : y\n",
    "X, y = [], []\n",
    "\n",
    "for fname in list(label_table.keys()):\n",
    "    feature_vector = []\n",
    "    label = label_table[fname.split('.')[0]]\n",
    "    for data in [peminer_url, ember_url, pestudio_url]:\n",
    "        path = f\"{data}/{fname}.json\"\n",
    "        if data == peminer_url:\n",
    "            feature_vector += PeminerParser(path).process_report()\n",
    "        elif data == ember_url:\n",
    "            feature_vector += EmberParser(path).process_report()\n",
    "        # else:\n",
    "        #     feature_vector += PestudioParser(path).process_report()\n",
    "    X.append(feature_vector)\n",
    "    y.append(label)\n",
    "\n",
    "np.asarray(X).shape, np.asarray(y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검증데이터 valid_X, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 558), (10000,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검증데이터\n",
    "perminer_check_url = (local_dir + '/PEMINER/검증데이터')\n",
    "ember_check_url = (local_dir + '/EMBER/검증데이터')\n",
    "pestudio_check_url = (local_dir + '/PESTUDIO/검증데이터')\n",
    "\n",
    "# 데이터의 특징 벡터 모음(2차원 리스트) : X\n",
    "# 데이터의 레이블 모음(1차원 리스트) : y\n",
    "valid_X, vaild_y = [], []\n",
    "\n",
    "for fname in list(check_label_table.keys()):\n",
    "    feature_vector = []\n",
    "    label = check_label_table[fname.split('.')[0]]\n",
    "    for data in [perminer_check_url, ember_check_url, pestudio_check_url]:\n",
    "        path = f\"{data}/{fname}.json\"\n",
    "        if data == perminer_check_url:\n",
    "            feature_vector += PeminerParser(path).process_report()\n",
    "        elif data == ember_check_url:\n",
    "            feature_vector += EmberParser(path).process_report()\n",
    "        # else:\n",
    "        #     feature_vector += PestudioParser(path).process_report()\n",
    "    valid_X.append(feature_vector)\n",
    "    vaild_y.append(label)\n",
    "\n",
    "np.asarray(valid_X).shape, np.asarray(vaild_y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 여러개 학습 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [02:36<00:00, 19.60s/it]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▎        | 1/8 [00:00<00:04,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest 정확도 :  0.9428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 2/8 [00:01<00:03,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree 정확도 :  0.9092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███▊      | 3/8 [00:01<00:03,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM 정확도 :  0.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 4/8 [00:46<01:11, 17.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM 정확도 :  0.8269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████▎   | 5/8 [00:46<00:35, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 정확도 :  0.8233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 6/8 [01:00<00:24, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN 정확도 :  0.9045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████▊ | 7/8 [01:01<00:08,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost 정확도 :  0.8938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:02<00:00,  7.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP 정확도 :  0.8382\n",
      "Finished evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model 여러개 학습\n",
    "model_name = [\"rf\", \"dt\", \"lgb\", \"svm\", \"lr\", \"knn\", \"adaboost\", \"mlp\"]\n",
    "models = []\n",
    "for model in tqdm(model_name):\n",
    "    model, clf = train(X, y, model)\n",
    "    models.append([model, clf])\n",
    "print('Finished training.')\n",
    "\n",
    "# 검증\n",
    "for model in tqdm(models): \n",
    "    evaluate(valid_X, vaild_y, model)\n",
    "print('Finished evaluation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앙상블 evaluate 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_result(X, y, models):\n",
    "    '''\n",
    "        학습된 모델들의 결과를 앙상블하는 함수\n",
    "\t\n",
    "        :param X: 검증할 2차원 리스트 특징 벡터\n",
    "        :param y: 검증할 1차원 리스트 레이블 벡터\n",
    "        :param models: 1개 이상의 학습된 머신러닝 모델 객체를 가지는 1차원 리스트\n",
    "    '''\n",
    "    \n",
    "    # Soft Voting\n",
    "    # https://devkor.tistory.com/entry/Soft-Voting-%EA%B3%BC-Hard-Voting\n",
    "    predicts = []\n",
    "    for model in tqdm(models):\n",
    "        prob = [result for _, result in model[1].predict_proba(X)]\n",
    "        predicts.append(prob)\n",
    "    \n",
    "    predict = np.mean(predicts, axis=0)\n",
    "    predict = [1 if x >= 0.5 else 0 for x in predict]\n",
    "        \n",
    "    print(\"emsemble 정확도 : \", accuracy_score(y, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앙상블 학습 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emsemble 정확도 :  0.9524\n",
      "Finished evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ensemble 학습\n",
    "models = []\n",
    "for model in [\"rf\", \"lgb\"]:\n",
    "    clf = train(X, y, model)\n",
    "    models.append(clf)\n",
    "print('Finished training.')\n",
    "\n",
    "# ensemble 검증\n",
    "ensemble_result(valid_X, vaild_y, models)\n",
    "print('Finished evaluation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q29vY5nuT2Hr"
   },
   "source": [
    "## 특징 선택 예제 (RFE 알고리즘 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feature(X, y, model):\n",
    "    '''\n",
    "        주어진 특징 벡터에서 특정 알고리즘 기반 특징 선택\n",
    "        \n",
    "        본 예제에서는 RFE 알고리즘 사용\n",
    "        https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE.fit_transform\n",
    "        \n",
    "        :param X: 검증할 2차원 리스트 특징 벡터\n",
    "        :param y: 검증할 1차원 리스트 레이블 벡터\n",
    "        :param model: 문자열, 특징 선택에 사용할 머신러닝 알고리즘\n",
    "    '''\n",
    "    \n",
    "    model, clf = load_model(model=model, random_state=SEED)\n",
    "    rfe = RFE(estimator=clf)\n",
    "    return rfe.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7c4092e0a426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mselected_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-257d7e68d1de>\u001b[0m in \u001b[0;36mselect_feature\u001b[0;34m(X, y, model)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mrfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRFE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_rfe.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \"\"\"\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_rfe.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, step_score)\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fitting estimator with %d features.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;31m# Get importance and rank them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m    388\u001b[0m                              \u001b[0;34m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'threads'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "selected_X = select_feature(X, y, \"rf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_X 학습\n",
    "new_model = train(selected_X, y, \"rf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_X 검증\n",
    "evaluate(selected_X, y, new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test 레이블 테이블 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testdata_filename():\n",
    "    return [i[:-5] for i in os.listdir(local_dir + '/EMBER/테스트데이터')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_table = get_testdata_filename()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test 데이터 test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:05<00:00, 1741.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 558)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터\n",
    "peminer_test_url = (local_dir + '/PEMINER/테스트데이터')\n",
    "ember_test_url = (local_dir + '/EMBER/테스트데이터')\n",
    "pestudio_test_url = (local_dir + '/PESTUDIO/테스트데이터')\n",
    "\n",
    "# 데이터의 특징 벡터 모음(2차원 리스트) : X\n",
    "test_X = []\n",
    "\n",
    "for fname in tqdm(test_label_table):\n",
    "    feature_vector = []\n",
    "    for data in [peminer_test_url, ember_test_url, pestudio_test_url]:\n",
    "        path = f\"{data}/{fname}.json\"\n",
    "        if data == peminer_test_url:\n",
    "            feature_vector += PeminerParser(path).process_report()\n",
    "        elif data == ember_test_url:\n",
    "            feature_vector += EmberParser(path).process_report()\n",
    "        # else:\n",
    "        #     feature_vector += PestudioParser(path).process_report()\n",
    "    test_X.append(feature_vector)\n",
    "\n",
    "np.asarray(test_X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_X 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_result(X, models):\n",
    "    '''\n",
    "        학습된 모델들의 결과를 앙상블하는 함수\n",
    "        :param models: 1개 이상의 학습된 머신러닝 모델 객체를 가지는 1차원 리스트\n",
    "    '''\n",
    "    \n",
    "    # Soft Voting\n",
    "    # https://devkor.tistory.com/entry/Soft-Voting-%EA%B3%BC-Hard-Voting\n",
    "    predicts = []\n",
    "    for model in models:\n",
    "        prob = [result for _, result in model[1].predict_proba(X)]\n",
    "        predicts.append(prob)\n",
    "    \n",
    "    predict = np.mean(predicts, axis=0)\n",
    "    predict = [1 if x >= 0.5 else 0 for x in predict]\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished evaluation.\n"
     ]
    }
   ],
   "source": [
    "predict = ensemble_result(test_X, models)\n",
    "# print(predict)\n",
    "print('Finished evaluation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test result csv 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_csv = pd.DataFrame(columns=['file', 'predict'])\n",
    "# predictions = pd.DataFrame(predict,columns = ['predict'])\n",
    "# test_frame = pd.DataFrame(get_testdata_filename())\n",
    "# submission_csv[\"predict\"] = predictions[\"predict\"]\n",
    "# submission_csv[\"file\"] = test_frame\n",
    "# submission_csv.to_csv(\"predict.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
